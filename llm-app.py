from huggingface_hub import InferenceClient
import psycopg2
import json
import tiktoken
import re
import os

# Database connection parameters
conn_params = {
    'dbname': 'cve',
    'user': 'postgres',
    'password': 'postgres',
    'host': 'postgres.consumer.svc.cluster.local',
    'port': '5432'
}

# Read environment variables
HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')
PROMPT_TEMPLATE = os.getenv('PROMPT_TEMPLATE')

# Function to fetch a single row from the database
def fetch_row(limit=1, offset=0):
    """Fetch a single row from the database."""
    conn = psycopg2.connect(**conn_params)
    cur = conn.cursor()
    query = "SELECT cve_references FROM your_table_name LIMIT %s OFFSET %s;"
    cur.execute(query, (limit, offset))
    row = cur.fetchone()
    cur.close()
    conn.close()
    return row

# Function to process a single row
def process_row(row, client):
    if row[0] is not None:
        # If row[0] is already a dict, no need to use json.loads
        if isinstance(row[0], dict):
            cve_reference = row[0]
        else:
            try:
                cve_reference = json.loads(row[0])
            except json.JSONDecodeError as e:
                print(f"JSON decode error: {e}")
                return None
        
        # Format the CVE data for the model
        cve_data = json.dumps(cve_reference, indent=2)
        prompt = PROMPT_TEMPLATE.format(cve_data=cve_data)

        # Estimate the token count for prompt and adjust
        encoder = tiktoken.get_encoding("gpt2")
        input_tokens = len(encoder.encode(prompt))
        max_input_tokens = 8192 - 1000  # Leaving room for output tokens
        if input_tokens > max_input_tokens:
            prompt = prompt[:max_input_tokens]

        response = client.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000,
            stream=False
        )
        return response.choices[0].message['content']
    return None

# Function to extract SQL queries from the LLM response
def extract_sql_queries(response):
    """Extract SQL queries from the LLM response."""
    queries = []
    # Using regex to extract SQL queries between ```sql``` blocks
    sql_blocks = re.findall(r'```sql(.*?)```', response, re.DOTALL)
    for block in sql_blocks:
        queries.append(block.strip())
    return queries

# Function to execute SQL queries
def execute_queries(queries):
    """Execute SQL queries on the PostgreSQL database."""
    conn = psycopg2.connect(**conn_params)
    cur = conn.cursor()
    for query in queries:
        try:
            cur.execute(query)
            conn.commit()
            print(f"Executed query: {query}")
        except psycopg2.Error as e:
            print(f"Error executing query: {e}")
            conn.rollback()
    cur.close()
    conn.close()

# Initialize InferenceClient
client = InferenceClient(
    "meta-llama/Meta-Llama-3.1-8B-Instruct",
    token=HUGGINGFACE_TOKEN
)

# Process data row by row
offset = 0
limit = 1  # Process one row at a time

while True:
    row = fetch_row(limit, offset)
    if not row:
        break

    response = process_row(row, client)
    if response:
        queries = extract_sql_queries(response)
        if queries:
            execute_queries(queries)

    offset += limit  # Move to the next row
